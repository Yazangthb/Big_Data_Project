{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b8b5050-2794-46c5-82ea-a788e634eced",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.metastore.uris\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/06 23:10:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/05/06 23:10:16 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/05/06 23:10:16 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/05/06 23:10:16 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "25/05/06 23:10:16 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "25/05/06 23:10:16 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "25/05/06 23:10:16 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n",
      "25/05/06 23:10:16 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.\n",
      "25/05/06 23:10:16 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.\n",
      "25/05/06 23:10:16 WARN Utils: Service 'SparkUI' could not bind on port 4048. Attempting port 4049.\n",
      "25/05/06 23:10:16 WARN Utils: Service 'SparkUI' could not bind on port 4049. Attempting port 4050.\n",
      "25/05/06 23:10:16 WARN Utils: Service 'SparkUI' could not bind on port 4050. Attempting port 4051.\n",
      "25/05/06 23:10:16 WARN Utils: Service 'SparkUI' could not bind on port 4051. Attempting port 4052.\n",
      "25/05/06 23:10:16 WARN Utils: Service 'SparkUI' could not bind on port 4052. Attempting port 4053.\n",
      "25/05/06 23:10:16 WARN Utils: Service 'SparkUI' could not bind on port 4053. Attempting port 4054.\n",
      "25/05/06 23:10:17 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n",
      "25/05/06 23:10:17 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Add here your team number teamx\n",
    "team = \"team17\"\n",
    "\n",
    "# location of your Hive database in HDFS\n",
    "warehouse = \"/user/team17/project/warehouse\"\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"{} - spark ML\".format(team))\\\n",
    "        .master(\"yarn\")\\\n",
    "        .config(\"hive.metastore.uris\", \"thrift://hadoop-02.uni.innopolis.ru:9883\")\\\n",
    "        .config(\"spark.sql.warehouse.dir\", warehouse)\\\n",
    "        .config(\"spark.sql.avro.compression.codec\", \"snappy\")\\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd6ab303-8955-4839-beca-a48721bc6944",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `train_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [train_tickets], [], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df = \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSELECT * FROM train_tickets\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m df.show()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyspark/sql/session.py:1631\u001b[39m, in \u001b[36mSparkSession.sql\u001b[39m\u001b[34m(self, sqlQuery, args, **kwargs)\u001b[39m\n\u001b[32m   1627\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1628\u001b[39m         litArgs = \u001b[38;5;28mself\u001b[39m._jvm.PythonUtils.toArray(\n\u001b[32m   1629\u001b[39m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[32m   1630\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1631\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jsparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1632\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1633\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    181\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `train_tickets` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [train_tickets], [], false\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"SELECT * FROM train_tickets\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e2bc130-606c-4a7e-8d14-bacbe4c0e75e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://hadoop-01.uni.innopolis.ru:4054\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>team17 - spark ML</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f08cd8aef10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46f01c36-cfc2-4eb1-bfa0-573cc2fb30c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           namespace|\n",
      "+--------------------+\n",
      "|             default|\n",
      "|             retake1|\n",
      "|             root_db|\n",
      "|                show|\n",
      "|     team0_projectdb|\n",
      "|    team11_projectdb|\n",
      "|           team12_db|\n",
      "|team12_hive_proje...|\n",
      "|    team12_projectdb|\n",
      "|    team13_projectdb|\n",
      "|    team14_projectdb|\n",
      "|    team15_projectdb|\n",
      "|    team16_projectdb|\n",
      "|    team17_projectdb|\n",
      "|    team18_projectdb|\n",
      "|    team19_projectdb|\n",
      "|     team1_projectdb|\n",
      "|    team20_projectdb|\n",
      "| team21_projectdb_v2|\n",
      "| team21_projectdb_v3|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "+----------------+--------------------+-----------+\n",
      "|       namespace|           tableName|isTemporary|\n",
      "+----------------+--------------------+-----------+\n",
      "|team17_projectdb|          q1_results|      false|\n",
      "|team17_projectdb|          q2_results|      false|\n",
      "|team17_projectdb|          q3_results|      false|\n",
      "|team17_projectdb|          q4_results|      false|\n",
      "|team17_projectdb|train_tickets_buc...|      false|\n",
      "|team17_projectdb|  train_tickets_part|      false|\n",
      "|team17_projectdb| train_tickets_part2|      false|\n",
      "|team17_projectdb|        traintickets|      false|\n",
      "+----------------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW DATABASES\").show()\n",
    "spark.sql(\"USE team17_projectdb\").show()\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "685eca2b-3310-4467-8b4e-7b51894e6d60",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseException",
     "evalue": "\n[PARSE_SYNTAX_ERROR] Syntax error at or near 'USE': extra input 'USE'.(line 1, pos 16)\n\n== SQL ==\nSHOW DATABASES; USE team17_projectb; show tables;\n----------------^^^\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mParseException\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSHOW DATABASES; USE team17_projectb; show tables;\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.show()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyspark/sql/session.py:1631\u001b[39m, in \u001b[36mSparkSession.sql\u001b[39m\u001b[34m(self, sqlQuery, args, **kwargs)\u001b[39m\n\u001b[32m   1627\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1628\u001b[39m         litArgs = \u001b[38;5;28mself\u001b[39m._jvm.PythonUtils.toArray(\n\u001b[32m   1629\u001b[39m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[32m   1630\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1631\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jsparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1632\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1633\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    181\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mParseException\u001b[39m: \n[PARSE_SYNTAX_ERROR] Syntax error at or near 'USE': extra input 'USE'.(line 1, pos 16)\n\n== SQL ==\nSHOW DATABASES; USE team17_projectb; show tables;\n----------------^^^\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW DATABASES; USE team17_projectb; show tables;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0500d38-4eb4-4dcf-8640-7643ea532dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.7 (main, Mar 20 2025, 00:23:21) [GCC 4.8.5 20150623 (Red Hat 4.8.5-44)] on linux\n",
      "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/06 00:05:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/05/06 00:05:40 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/05/06 00:05:40 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/05/06 00:05:40 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "25/05/06 00:05:40 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "25/05/06 00:05:40 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "25/05/06 00:05:40 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n",
      "25/05/06 00:05:40 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.\n",
      "25/05/06 00:05:40 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.\n",
      "25/05/06 00:05:41 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n",
      "25/05/06 00:05:41 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.5.5\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.11.7 (main, Mar 20 2025 00:23:21)\n",
      "Spark context Web UI available at http://hadoop-01.uni.innopolis.ru:4048\n",
      "Spark context available as 'sc' (master = yarn, app id = application_1745788519616_2945).\n",
      "SparkSession available as 'spark'.\n",
      ">>> ^C\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 0, in <module>\n",
      "  File \"/usr/local/lib/python3.11/site-packages/pyspark/context.py\", line 382, in signal_handler\n",
      "    raise KeyboardInterrupt()\n",
      "KeyboardInterrupt\n",
      ">>> "
     ]
    }
   ],
   "source": [
    "!pyspark --master yarn --conf \"spark.driver.extraJavaOptions=-Dhive.metastore.uris=thrift://hadoop-02.uni.innopolis.ru:9883\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c722abe9-a31a-4d32-a62e-2dab08a95aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           namespace|\n",
      "+--------------------+\n",
      "|             default|\n",
      "|             retake1|\n",
      "|             root_db|\n",
      "|                show|\n",
      "|     team0_projectdb|\n",
      "|    team11_projectdb|\n",
      "|           team12_db|\n",
      "|team12_hive_proje...|\n",
      "|    team12_projectdb|\n",
      "|    team13_projectdb|\n",
      "|    team14_projectdb|\n",
      "|    team15_projectdb|\n",
      "|    team16_projectdb|\n",
      "|    team17_projectdb|\n",
      "|    team18_projectdb|\n",
      "|    team19_projectdb|\n",
      "|     team1_projectdb|\n",
      "|    team20_projectdb|\n",
      "| team21_projectdb_v2|\n",
      "| team21_projectdb_v3|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "+----------------+--------------------+-----------+\n",
      "|       namespace|           tableName|isTemporary|\n",
      "+----------------+--------------------+-----------+\n",
      "|team17_projectdb|          q1_results|      false|\n",
      "|team17_projectdb|          q2_results|      false|\n",
      "|team17_projectdb|          q3_results|      false|\n",
      "|team17_projectdb|          q4_results|      false|\n",
      "|team17_projectdb|train_tickets_buc...|      false|\n",
      "|team17_projectdb|  train_tickets_part|      false|\n",
      "|team17_projectdb| train_tickets_part2|      false|\n",
      "|team17_projectdb|        traintickets|      false|\n",
      "+----------------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW DATABASES\").show()\n",
    "spark.sql(\"USE team17_projectdb\").show()\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6bfabff-0ddb-411a-b3eb-077cac7a688e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------------+-------------+--------+------------+------------------+-----+----------+------+\n",
      "|      id|destination|    departure|      arrival|duration|vehicle_type|     vehicle_class|price|      fare|origin|\n",
      "+--------+-----------+-------------+-------------+--------+------------+------------------+-----+----------+------+\n",
      "|26515944| PONFERRADA|1589380200000|1589394780000|    4.05|       ALVIA|           Turista| 31.0|   Promo +|MADRID|\n",
      "|27266781|   ZARAGOZA|1584523800000|1584528300000|    1.25|         AVE|           Unknown| 55.7|  Standard|MADRID|\n",
      "|26020141| VALLADOLID|1588827120000|1588831020000|    1.08|       AVANT|           Turista| 31.1|Adulto ida|MADRID|\n",
      "|30156853|     LLEIDA|1592329500000|1592337000000|    2.08|         AVE|           Unknown| 74.7|  Standard|MADRID|\n",
      "| 8500064|   VALENCIA|1559918880000|1559933880000|    4.17|   INTERCITY|      Turista Plus| 30.9|     Promo|MADRID|\n",
      "|35007203|   ALBACETE|1597386540000|1597397220000|    2.97|          MD|Turista con enlace| 23.6|  Flexible|MADRID|\n",
      "| 5654258|    SEVILLA|1558011600000|1558020600000|     2.5|         AVE|           Turista| 76.3|  Flexible|MADRID|\n",
      "|   53944|  BARCELONA|1559062800000|1559071800000|     2.5|         AVE|           Turista|88.95|     Promo|MADRID|\n",
      "|33068376|    CORDOBA|1590224400000|1590230580000|    1.72|         AVE|           Unknown| 63.4|  Standard|MADRID|\n",
      "|21103479| VALLADOLID|1583649600000|1583660100000|    2.92|          MD|           Turista|24.95|Adulto ida|MADRID|\n",
      "|13477577|    SEVILLA|1566403200000|1566412680000|    2.63|         AVE|           Turista| 41.2|     Promo|MADRID|\n",
      "|11345204|  BARCELONA|1560568800000|1560580140000|    3.15|         AVE|           Turista|107.7|  Flexible|MADRID|\n",
      "|31293483|  TARRAGONA|1588235400000|1588244580000|    2.55|         AVE|           Unknown| 82.6|  Standard|MADRID|\n",
      "| 9551065| PONFERRADA|1559994000000|1560009120000|     4.2|          LD|Turista con enlace| 39.9|     Promo|MADRID|\n",
      "|18518532|    SEVILLA|1583416800000|1583426280000|    2.63|         AVE|           Turista| 77.1|  Flexible|MADRID|\n",
      "|31064811| VALLADOLID|1590995700000|1590999600000|    1.08|       AVANT|           Unknown| 31.1|  Standard|MADRID|\n",
      "|13452874|    SEVILLA|1566201600000|1566210720000|    2.53|         AVE|           Turista| 47.3|     Promo|MADRID|\n",
      "|18277954|  BARCELONA|1584019800000|1584029700000|    2.75|         AVE|           Turista|108.9|  Flexible|MADRID|\n",
      "| 8415072|    SEVILLA|1558963800000|1558972260000|    2.35|         AVE|           Turista| 76.3|  Flexible|MADRID|\n",
      "|15655726|    SEVILLA|1576906200000|1576916040000|    2.73|       ALVIA|           Turista| 67.2|  Flexible|MADRID|\n",
      "+--------+-----------+-------------+-------------+--------+------------+------------------+-----+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM team17_projectdb.train_tickets_part\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32b7abb4-9bd0-419d-914d-0bb3e73cb7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           namespace|\n",
      "+--------------------+\n",
      "|             default|\n",
      "|             retake1|\n",
      "|             root_db|\n",
      "|                show|\n",
      "|     team0_projectdb|\n",
      "|    team11_projectdb|\n",
      "|           team12_db|\n",
      "|team12_hive_proje...|\n",
      "|    team12_projectdb|\n",
      "|    team13_projectdb|\n",
      "|    team14_projectdb|\n",
      "|    team15_projectdb|\n",
      "|    team16_projectdb|\n",
      "|    team17_projectdb|\n",
      "|    team18_projectdb|\n",
      "|    team19_projectdb|\n",
      "|     team1_projectdb|\n",
      "|    team20_projectdb|\n",
      "| team21_projectdb_v2|\n",
      "| team21_projectdb_v3|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW DATABASES;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "03d9b719-ca7c-455b-930e-5d29499a42ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o138.listTables.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.lang.Thread.run(Thread.java:750)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\n\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2702)\n\tat org.apache.spark.sql.SparkSession.$anonfun$leafNodeDefaultParallelism$1(SparkSession.scala:906)\n\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.SparkSession.leafNodeDefaultParallelism(SparkSession.scala:906)\n\tat org.apache.spark.sql.execution.CommandResultExec.rdd$lzycompute(CommandResultExec.scala:57)\n\tat org.apache.spark.sql.execution.CommandResultExec.rdd(CommandResultExec.scala:52)\n\tat org.apache.spark.sql.execution.CommandResultExec.doExecute(CommandResultExec.scala:64)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:207)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:206)\n\tat org.apache.spark.sql.internal.CatalogImpl.makeTablesDataset(CatalogImpl.scala:156)\n\tat org.apache.spark.sql.internal.CatalogImpl.listTablesInternal(CatalogImpl.scala:146)\n\tat org.apache.spark.sql.internal.CatalogImpl.listTables(CatalogImpl.scala:128)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcatalog\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistTables\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mteam17_projectdb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyspark/sql/catalog.py:361\u001b[39m, in \u001b[36mCatalog.listTables\u001b[39m\u001b[34m(self, dbName, pattern)\u001b[39m\n\u001b[32m    358\u001b[39m     dbName = \u001b[38;5;28mself\u001b[39m.currentDatabase()\n\u001b[32m    360\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pattern \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m361\u001b[39m     \u001b[38;5;28miter\u001b[39m = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jcatalog\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistTables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdbName\u001b[49m\u001b[43m)\u001b[49m.toLocalIterator()\n\u001b[32m    362\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    363\u001b[39m     \u001b[38;5;28miter\u001b[39m = \u001b[38;5;28mself\u001b[39m._jcatalog.listTables(dbName, pattern).toLocalIterator()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o138.listTables.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.lang.Thread.run(Thread.java:750)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\n\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2702)\n\tat org.apache.spark.sql.SparkSession.$anonfun$leafNodeDefaultParallelism$1(SparkSession.scala:906)\n\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.SparkSession.leafNodeDefaultParallelism(SparkSession.scala:906)\n\tat org.apache.spark.sql.execution.CommandResultExec.rdd$lzycompute(CommandResultExec.scala:57)\n\tat org.apache.spark.sql.execution.CommandResultExec.rdd(CommandResultExec.scala:52)\n\tat org.apache.spark.sql.execution.CommandResultExec.doExecute(CommandResultExec.scala:64)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:207)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:206)\n\tat org.apache.spark.sql.internal.CatalogImpl.makeTablesDataset(CatalogImpl.scala:156)\n\tat org.apache.spark.sql.internal.CatalogImpl.listTablesInternal(CatalogImpl.scala:146)\n\tat org.apache.spark.sql.internal.CatalogImpl.listTables(CatalogImpl.scala:128)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "print(spark.catalog.listTables(\"team17_projectdb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9d24f880-e3b2-4417-b7e1-b3cfee89fe56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[namespace: string, tableName: string, isTemporary: boolean]\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"USE team17_projectdb;\")\n",
    "print(spark.sql(\"SHOW TABLES;\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8087cdb-c6c5-4bf7-8f5a-c9c405a07ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickets = spark.read.format(\"avro\").table('team17_projectdb.train_tickets_part2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b57a8c16-f09e-41c9-bfa7-df5d7af50e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- destination: string (nullable = true)\n",
      " |-- departure: long (nullable = true)\n",
      " |-- arrival: long (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- fare: string (nullable = true)\n",
      " |-- vehicle_type: string (nullable = true)\n",
      " |-- vehicle_class: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tickets.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "53c0539a-dbbf-4c42-8d68-bdbc7d0fbb97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+-------------+-------------+--------+-----+--------+------------+-------------+\n",
      "|      id|   origin|destination|    departure|      arrival|duration|price|    fare|vehicle_type|vehicle_class|\n",
      "+--------+---------+-----------+-------------+-------------+--------+-----+--------+------------+-------------+\n",
      "|22912762| ZARAGOZA|     MADRID|1585330860000|1585335600000|    1.32| 55.7|Flexible|         AVE|      Turista|\n",
      "|15697301|  SEVILLA|     MADRID|1576251900000|1576261020000|    2.53| 47.3|   Promo|         AVE|      Turista|\n",
      "| 5654258|   MADRID|    SEVILLA|1558011600000|1558020600000|     2.5| 76.3|Flexible|         AVE|      Turista|\n",
      "|   53944|   MADRID|  BARCELONA|1559062800000|1559071800000|     2.5|88.95|   Promo|         AVE|      Turista|\n",
      "|24449885|   LLEIDA|     MADRID|1583773080000|1583780400000|    2.03| 74.7|Flexible|         AVE|      Turista|\n",
      "|19376873| ZARAGOZA|     MADRID|1584520980000|1584526200000|    1.45| 55.7|Flexible|         AVE|      Turista|\n",
      "|36332040|TARRAGONA|     MADRID|1599449940000|1599459600000|    2.68|48.75|   YOVOY|         AVE|      Turista|\n",
      "|11746445| VALENCIA|     MADRID|1563961200000|1563967200000|    1.67|33.65|   Promo|         AVE|      Turista|\n",
      "| 1829733| VALENCIA|     MADRID|1557663000000|1557669780000|    1.88|51.15|   Promo|         AVE|      Turista|\n",
      "|16111137|  SEVILLA|     MADRID|1584092400000|1584102180000|    2.72| 47.8|   Promo|         AVE|      Turista|\n",
      "+--------+---------+-----------+-------------+-------------+--------+-----+--------+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM train_tickets_part2 LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90211c4b-7e3a-4074-adbf-27e94b580b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "# Load the data\n",
    "# tickets = spark.table(\"train_tickets_part2\")\n",
    "\n",
    "# Convert UNIX timestamps to human-readable timestamps\n",
    "tickets = tickets.withColumn(\"departure_time\", F.from_unixtime(F.col(\"departure\")/1000).cast(TimestampType()))\n",
    "tickets = tickets.withColumn(\"arrival_time\", F.from_unixtime(F.col(\"arrival\")/1000).cast(TimestampType()))\n",
    "\n",
    "# Extract useful time features\n",
    "tickets = tickets.withColumn(\"departure_hour\", F.hour(\"departure_time\"))\n",
    "tickets = tickets.withColumn(\"departure_day_of_week\", F.dayofweek(\"departure_time\"))\n",
    "tickets = tickets.withColumn(\"is_weekend\", F.when(F.dayofweek(\"departure_time\").isin([1,7]), 1).otherwise(0))\n",
    "tickets = tickets.withColumn(\"trip_hours\", F.col(\"duration\"))\n",
    "\n",
    "# Define features and label\n",
    "features = [\n",
    "    'origin', 'destination', \n",
    "    'trip_hours', 'vehicle_type', 'vehicle_class',\n",
    "    'departure_hour', 'departure_day_of_week',  'fare'\n",
    "]\n",
    "label = 'price'\n",
    "\n",
    "# Clean data\n",
    "tickets = tickets.select(features + [label]).na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c869bd0c-2588-450e-aebf-3b0581c917d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import (StringIndexer, OneHotEncoder, \n",
    "                               VectorAssembler, StandardScaler,\n",
    "                               PolynomialExpansion)\n",
    "\n",
    "# Categorical columns\n",
    "categoricalCols = ['origin', 'destination', 'vehicle_type', 'vehicle_class' , 'fare']\n",
    "\n",
    "# Numerical columns (including our new time features)\n",
    "numericalCols = ['trip_hours', 'departure_hour', 'departure_day_of_week']\n",
    "\n",
    "# Create StringIndexers for categorical columns\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=f\"{c}_index\", handleInvalid=\"keep\") \n",
    "            for c in categoricalCols]\n",
    "\n",
    "# Create OneHotEncoders\n",
    "encoders = [OneHotEncoder(inputCol=f\"{c}_index\", outputCol=f\"{c}_encoded\") \n",
    "            for c in categoricalCols]\n",
    "\n",
    "# Scale numerical features\n",
    "numerical_assembler = VectorAssembler(inputCols=numericalCols, outputCol=\"numerical_features\")\n",
    "scaler = StandardScaler(inputCol=\"numerical_features\", outputCol=\"scaled_numerical\")\n",
    "\n",
    "# Final feature assembler\n",
    "\n",
    "# Create pipeline\n",
    "linear_pipeline = Pipeline(stages=indexers + encoders + [numerical_assembler, scaler])\n",
    "poly_expansion = PolynomialExpansion(degree=2, inputCol=\"scaled_numerical\", outputCol=\"poly_features\")\n",
    "\n",
    "\n",
    "poly_pipeline=Pipeline(stages=linear_pipeline.getStages() + [poly_expansion])\n",
    "                       \n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[f\"{c}_encoded\" for c in categoricalCols] + [\"scaled_numerical\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "poly_assembler = VectorAssembler(\n",
    "    inputCols=[f\"{c}_encoded\" for c in categoricalCols] + [\"poly_features\"],\n",
    "    outputCol=\"features\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fdb122d6-2964-423d-8e20-0089a3e23b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import (StringIndexer, OneHotEncoder, \n",
    "                               VectorAssembler, StandardScaler,\n",
    "                               PolynomialExpansion)\n",
    "\n",
    "# 1. Common initial steps (shared by all pipelines)\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=f\"{c}_index\", handleInvalid=\"keep\") \n",
    "            for c in categoricalCols]\n",
    "\n",
    "# 2. Create different pipeline branches\n",
    "# --- Branch 1: Simple (indexers + numerical) ---\n",
    "simple_assembler = VectorAssembler(\n",
    "    inputCols=[f\"{c}_index\" for c in categoricalCols] + numericalCols,\n",
    "    outputCol=\"features_simple\"\n",
    ")\n",
    "\n",
    "# --- Branch 2: Standard (indexers + one-hot + scaled numerical) ---\n",
    "encoders = [OneHotEncoder(inputCol=f\"{c}_index\", outputCol=f\"{c}_encoded\") \n",
    "           for c in categoricalCols]\n",
    "\n",
    "numerical_assembler = VectorAssembler(inputCols=numericalCols, outputCol=\"numerical_features\")\n",
    "scaler = StandardScaler(inputCol=\"numerical_features\", outputCol=\"scaled_numerical\")\n",
    "\n",
    "standard_assembler = VectorAssembler(\n",
    "    inputCols=[f\"{c}_encoded\" for c in categoricalCols] + [\"scaled_numerical\"],\n",
    "    outputCol=\"features_standard\"\n",
    ")\n",
    "\n",
    "# --- Branch 3: Polynomial (standard + polynomial expansion) ---\n",
    "poly_expansion = PolynomialExpansion(degree=2, inputCol=\"scaled_numerical\", outputCol=\"poly_features\")\n",
    "poly_assembler = VectorAssembler(\n",
    "    inputCols=[f\"{c}_encoded\" for c in categoricalCols] + [\"poly_features\"],\n",
    "    outputCol=\"features_poly\"\n",
    ")\n",
    "\n",
    "# 3. Final unified pipeline\n",
    "full_pipeline = Pipeline(stages=indexers + [\n",
    "    simple_assembler,\n",
    "    *encoders,\n",
    "    numerical_assembler,\n",
    "    scaler,\n",
    "    standard_assembler,\n",
    "    poly_expansion,\n",
    "    poly_assembler\n",
    "])\n",
    "\n",
    "# 4. Fit once, get all versions\n",
    "# pipeline_model = full_pipeline.fit(train_data)\n",
    "# multi_transformed = pipeline_model.transform(train_data)\n",
    "\n",
    "# Now you have all feature versions in one DataFrame:\n",
    "# - multi_transformed.select(\"features_simple\") → For simple model\n",
    "# - multi_transformed.select(\"features_standard\") → For standard linear regression\n",
    "# - multi_transformed.select(\"features_poly\") → For polynomial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9b46f2a-955a-40d7-acd8-15ebbf5b229d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "train_data, test_data = tickets.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "pipeline_model = full_pipeline.fit(train_data)\n",
    "multi_transformed = pipeline_model.transform(train_data)\n",
    "multi_transformed_test = pipeline_model.transform(test_data)\n",
    "# 2. Fit the base pipeline (linear features)\n",
    "# linear_pipeline_model = linear_pipeline.fit(train_data)\n",
    "# train_linear = linear_pipeline_model.transform(train_data)\n",
    "# test_linear = linear_pipeline_model.transform(test_data)\n",
    "\n",
    "# 3. Apply final assembler for linear model\n",
    "# train_linear_final = multi_transformed.select(\"features_standard\")\n",
    "# test_linear_final = multi_transformed_test.select(\"features_standard\")\n",
    "\n",
    "\n",
    "# Train Linear Regression\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(\n",
    "    featuresCol=\"features_standard\",\n",
    "    labelCol=\"price\",\n",
    "    maxIter=20,\n",
    "    regParam=0.3,\n",
    "    elasticNetParam=0.8\n",
    ")\n",
    "\n",
    "lr_model = lr.fit(multi_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24113c82-89d4-461e-868b-38370e86b38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 10.51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+--------+-----------+----------+------------+-------------+--------------+---------------------+--------+\n",
      "|price|prediction        |origin  |destination|trip_hours|vehicle_type|vehicle_class|departure_hour|departure_day_of_week|fare    |\n",
      "+-----+------------------+--------+-----------+----------+------------+-------------+--------------+---------------------+--------+\n",
      "|39.6 |48.74942727399128 |ALBACETE|MADRID     |1.5       |AVE         |Preferente   |8             |2                    |Promo + |\n",
      "|41.35|47.39643438479799 |ALBACETE|MADRID     |1.5       |AVE         |Preferente   |8             |5                    |Promo   |\n",
      "|36.65|47.39643438479799 |ALBACETE|MADRID     |1.5       |AVE         |Preferente   |8             |7                    |Promo   |\n",
      "|36.65|47.39643438479799 |ALBACETE|MADRID     |1.5       |AVE         |Preferente   |8             |7                    |Promo   |\n",
      "|41.35|47.39643438479799 |ALBACETE|MADRID     |1.5       |AVE         |Preferente   |8             |7                    |Promo   |\n",
      "|44.35|48.74942727399128 |ALBACETE|MADRID     |1.5       |AVE         |Preferente   |8             |7                    |Promo + |\n",
      "|44.35|48.74942727399128 |ALBACETE|MADRID     |1.5       |AVE         |Preferente   |8             |7                    |Promo + |\n",
      "|44.35|48.74942727399128 |ALBACETE|MADRID     |1.5       |AVE         |Preferente   |8             |7                    |Promo + |\n",
      "|45.5 |52.775589083349594|ALBACETE|MADRID     |1.5       |AVE         |Turista      |8             |2                    |Flexible|\n",
      "|45.5 |52.775589083349594|ALBACETE|MADRID     |1.5       |AVE         |Turista      |8             |2                    |Flexible|\n",
      "+-----+------------------+--------+-----------+----------+------------+-------------+--------------+---------------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Get feature names (including our new time features)\u001b[39;00m\n\u001b[32m     23\u001b[39m feature_names = (\n\u001b[32m     24\u001b[39m     [\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_encoded\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m categoricalCols] + \n\u001b[32m     25\u001b[39m     numericalCols\n\u001b[32m     26\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m coeff_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mFeature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mCoefficient\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcoefficients\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtoArray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m}\u001b[49m\u001b[43m)\u001b[49m.sort_values(\u001b[33m\"\u001b[39m\u001b[33mCoefficient\u001b[39m\u001b[33m\"\u001b[39m, ascending=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mFeature Importance:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(coeff_df)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pandas/core/frame.py:778\u001b[39m, in \u001b[36mDataFrame.__init__\u001b[39m\u001b[34m(self, data, index, columns, dtype, copy)\u001b[39m\n\u001b[32m    772\u001b[39m     mgr = \u001b[38;5;28mself\u001b[39m._init_mgr(\n\u001b[32m    773\u001b[39m         data, axes={\u001b[33m\"\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m\"\u001b[39m: index, \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: columns}, dtype=dtype, copy=copy\n\u001b[32m    774\u001b[39m     )\n\u001b[32m    776\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    777\u001b[39m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m778\u001b[39m     mgr = \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma.MaskedArray):\n\u001b[32m    780\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mma\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pandas/core/internals/construction.py:503\u001b[39m, in \u001b[36mdict_to_mgr\u001b[39m\u001b[34m(data, index, columns, dtype, typ, copy)\u001b[39m\n\u001b[32m    499\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    500\u001b[39m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[32m    501\u001b[39m         arrays = [x.copy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pandas/core/internals/construction.py:114\u001b[39m, in \u001b[36marrays_to_mgr\u001b[39m\u001b[34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[32m    112\u001b[39m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m         index = \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    116\u001b[39m         index = ensure_index(index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pandas/core/internals/construction.py:677\u001b[39m, in \u001b[36m_extract_index\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m    675\u001b[39m lengths = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[32m    676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m677\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mAll arrays must be of the same length\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    679\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[32m    680\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    681\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    682\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "predictions = lr_model.transform(test_linear_final)\n",
    "\n",
    "# Evaluate\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"price\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "\n",
    "# Show sample predictions\n",
    "predictions.select(\"price\", \"prediction\", *features).show(10, truncate=False)\n",
    "\n",
    "# Feature importance\n",
    "import pandas as pd\n",
    "\n",
    "# Get feature names (including our new time features)\n",
    "feature_names = (\n",
    "    [f\"{c}_encoded\" for c in categoricalCols] + \n",
    "    numericalCols\n",
    ")\n",
    "\n",
    "coeff_df = pd.DataFrame({\n",
    "    \"Feature\": feature_names,\n",
    "    \"Coefficient\": lr_model.coefficients.toArray()\n",
    "}).sort_values(\"Coefficient\", ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importance:\")\n",
    "print(coeff_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a0e0a84b-5b00-46de-b483-ffec386351a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import (StringIndexer, OneHotEncoder, \n",
    "                               VectorAssembler, StandardScaler,\n",
    "                               PolynomialExpansion)\n",
    "ndexers = [StringIndexer(inputCol=c, outputCol=f\"{c}_index\", handleInvalid=\"keep\") \n",
    "            for c in categoricalCols]\n",
    "\n",
    "# Create OneHotEncoders\n",
    "encoders = [OneHotEncoder(inputCol=f\"{c}_index\", outputCol=f\"{c}_encoded\") \n",
    "            for c in categoricalCols]\n",
    "\n",
    "# Assemble and scale numerical features\n",
    "numerical_assembler = VectorAssembler(inputCols=numericalCols, outputCol=\"numerical_features\")\n",
    "scaler = StandardScaler(inputCol=\"numerical_features\", outputCol=\"scaled_numerical\")\n",
    "\n",
    "# Polynomial expansion (degree=2 for quadratic features)\n",
    "poly_expansion = PolynomialExpansion(degree=2, inputCol=\"scaled_numerical\", outputCol=\"poly_features\")\n",
    "\n",
    "# Final feature assembler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[f\"{c}_encoded\" for c in categoricalCols] + [\"poly_features\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline(stages=indexers + encoders + [numerical_assembler, scaler, poly_expansion, assembler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42fa7f55-fe97-44c2-a5b5-e00645b352c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "# train_data, test_data = tickets.randomSplit([0.7, 0.3], seed=42)\n",
    "poly_pipeline_model = poly_pipeline.fit(train_data)\n",
    "train_poly = poly_pipeline_model.transform(train_data)\n",
    "test_poly = poly_pipeline_model.transform(test_data)\n",
    "\n",
    "# 5. Apply polynomial assembler\n",
    "train_poly_final = poly_assembler.transform(train_poly)\n",
    "test_poly_final = poly_assembler.transform(test_poly)\n",
    "\n",
    "# Fit pipeline\n",
    "# preprocessing_model = pipeline.fit(train_data)\n",
    "# train_transformed = preprocessing_model.transform(train_data)\n",
    "# test_transformed = preprocessing_model.transform(test_data)\n",
    "\n",
    "# Train Linear Regression (now with polynomial features)\n",
    "\n",
    "\n",
    "lr = LinearRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"price\",\n",
    "    maxIter=100,  # May need more iterations for complex features\n",
    "    regParam=0.1,  # Regularization to prevent overfitting\n",
    "    elasticNetParam=0.5  # Balance between L1 and L2 regularization\n",
    ")\n",
    "\n",
    "lr_model = lr.fit(train_poly_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a32d7dc-0e52-48b9-90ab-549da8a85d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 9.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared: 0.8271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 54:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+--------+-----------+----------+------------+-------------+--------------+---------------------+--------+\n",
      "|price|prediction       |origin  |destination|trip_hours|vehicle_type|vehicle_class|departure_hour|departure_day_of_week|fare    |\n",
      "+-----+-----------------+--------+-----------+----------+------------+-------------+--------------+---------------------+--------+\n",
      "|39.6 |46.72150827320125|ALBACETE|MADRID     |1.5       |AVE         |Preferente   |8             |2                    |Promo + |\n",
      "|41.35|43.58155101484428|ALBACETE|MADRID     |1.5       |AVE         |Preferente   |8             |5                    |Promo   |\n",
      "|36.65|43.47073868654652|ALBACETE|MADRID     |1.5       |AVE         |Preferente   |8             |7                    |Promo   |\n",
      "|36.65|43.47073868654652|ALBACETE|MADRID     |1.5       |AVE         |Preferente   |8             |7                    |Promo   |\n",
      "|41.35|43.47073868654652|ALBACETE|MADRID     |1.5       |AVE         |Preferente   |8             |7                    |Promo   |\n",
      "|44.35|46.44447745245684|ALBACETE|MADRID     |1.5       |AVE         |Preferente   |8             |7                    |Promo + |\n",
      "|44.35|46.44447745245684|ALBACETE|MADRID     |1.5       |AVE         |Preferente   |8             |7                    |Promo + |\n",
      "|44.35|46.44447745245684|ALBACETE|MADRID     |1.5       |AVE         |Preferente   |8             |7                    |Promo + |\n",
      "|45.5 |50.31334503792409|ALBACETE|MADRID     |1.5       |AVE         |Turista      |8             |2                    |Flexible|\n",
      "|45.5 |50.31334503792409|ALBACETE|MADRID     |1.5       |AVE         |Turista      |8             |2                    |Flexible|\n",
      "+-----+-----------------+--------+-----------+----------+------------+-------------+--------------+---------------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "predictions = lr_model.transform(test_poly_final)\n",
    "\n",
    "# Evaluate\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"price\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "\n",
    "# R-squared metric\n",
    "r2 = evaluator.setMetricName(\"r2\").evaluate(predictions)\n",
    "print(f\"R-squared: {r2:.4f}\")\n",
    "\n",
    "# Show sample predictions\n",
    "predictions.select(\"price\", \"prediction\", *features).show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c6328ec6-4328-48a5-a4f7-f530fb24b727",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 43:===================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline RMSE (always predict mean): 23.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "mean_price = tickets.select(F.mean('price')).first()[0]\n",
    "\n",
    "# Create a \"dumb\" model that always predicts the mean\n",
    "baseline_predictions = test_transformed.withColumn(\"prediction\", lit(mean_price))\n",
    "baseline_rmse = evaluator.evaluate(baseline_predictions)\n",
    "\n",
    "print(f\"Baseline RMSE (always predict mean): {baseline_rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dd928e9f-66ea-456e-8bae-da0e22c8d3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexers = [StringIndexer(inputCol=c, outputCol=f\"{c}_index\", handleInvalid=\"keep\") \n",
    "            for c in categoricalCols]\n",
    "\n",
    "# Final feature assembler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[f\"{c}_index\" for c in categoricalCols] + numericalCols,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline(stages=indexers + [assembler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3416a7f4-c816-485c-a0ee-1e456f3d7864",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "train_data, test_data = tickets.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# Fit pipeline\n",
    "preprocessing_model = pipeline.fit(train_data)\n",
    "train_transformed = preprocessing_model.transform(train_data)\n",
    "test_transformed = preprocessing_model.transform(test_data)\n",
    "\n",
    "# Train Decision Tree\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "dt = DecisionTreeRegressor(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"price\",\n",
    "    maxDepth=5,          # Control tree depth to prevent overfitting\n",
    "    minInstancesPerNode=10, # Minimum samples per leaf node\n",
    "    maxBins=32          # For categorical variables\n",
    ")\n",
    "\n",
    "dt_model = dt.fit(train_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ea83ae68-6b4d-497a-a9ab-8c36e0454930",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 11.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared: 0.7501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 107:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+--------+-----------+----------+------------+-------------+--------------+---------------------+--------+\n",
      "|price|prediction        |origin  |destination|trip_hours|vehicle_type|vehicle_class|departure_hour|departure_day_of_week|fare    |\n",
      "+-----+------------------+--------+-----------+----------+------------+-------------+--------------+---------------------+--------+\n",
      "|39.6 |60.44584890914875 |ALBACETE|MADRID     |1.5       |AVE         |Preferente   |8             |2                    |Promo + |\n",
      "|41.35|60.44584890914875 |ALBACETE|MADRID     |1.5       |AVE         |Preferente   |8             |5                    |Promo   |\n",
      "|36.65|49.77920434630192 |ALBACETE|MADRID     |1.5       |AVE         |Preferente   |8             |7                    |Promo   |\n",
      "|36.65|49.77920434630192 |ALBACETE|MADRID     |1.5       |AVE         |Preferente   |8             |7                    |Promo   |\n",
      "|41.35|49.77920434630192 |ALBACETE|MADRID     |1.5       |AVE         |Preferente   |8             |7                    |Promo   |\n",
      "|44.35|49.77920434630192 |ALBACETE|MADRID     |1.5       |AVE         |Preferente   |8             |7                    |Promo + |\n",
      "|44.35|49.77920434630192 |ALBACETE|MADRID     |1.5       |AVE         |Preferente   |8             |7                    |Promo + |\n",
      "|44.35|49.77920434630192 |ALBACETE|MADRID     |1.5       |AVE         |Preferente   |8             |7                    |Promo + |\n",
      "|45.5 |38.494845876704225|ALBACETE|MADRID     |1.5       |AVE         |Turista      |8             |2                    |Flexible|\n",
      "|45.5 |38.494845876704225|ALBACETE|MADRID     |1.5       |AVE         |Turista      |8             |2                    |Flexible|\n",
      "+-----+------------------+--------+-----------+----------+------------+-------------+--------------+---------------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "predictions = dt_model.transform(test_transformed)\n",
    "\n",
    "# Evaluate\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"price\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "\n",
    "# R-squared metric\n",
    "r2 = evaluator.setMetricName(\"r2\").evaluate(predictions)\n",
    "print(f\"R-squared: {r2:.4f}\")\n",
    "\n",
    "# Show sample predictions\n",
    "predictions.select(\"price\", \"prediction\", *features).show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d702ac62-0e1c-4516-970d-40174bbedb3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3.11",
   "language": "python",
   "name": "python3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
